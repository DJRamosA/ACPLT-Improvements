{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dra98\\OneDrive\\Documentos\\Trabajo\\Doctorado\\Codigo\\myvenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "\n",
    "\n",
    "# Word Embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Created functions\n",
    "from Experimentations import ParamSearch\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_length = 768"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Palabra (concepto)</th>\n",
       "      <th>Descripción</th>\n",
       "      <th>Codificación</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Compasión</td>\n",
       "      <td>sentimiento</td>\n",
       "      <td>sentimiento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Compasión</td>\n",
       "      <td>por él que él él perdonar el vida a alguien en...</td>\n",
       "      <td>perdón</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>plan</td>\n",
       "      <td>necesario para cumplir objetivo</td>\n",
       "      <td>objetivos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>plan</td>\n",
       "      <td>organizar recurso o persona</td>\n",
       "      <td>organización</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>plan</td>\n",
       "      <td>estrategia</td>\n",
       "      <td>estrategia</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4933</th>\n",
       "      <td>Obligación</td>\n",
       "      <td>imposicion</td>\n",
       "      <td>imposición</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4934</th>\n",
       "      <td>Obligación</td>\n",
       "      <td>carácter extricto</td>\n",
       "      <td>extricto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4935</th>\n",
       "      <td>Obligación</td>\n",
       "      <td>norma</td>\n",
       "      <td>normas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4936</th>\n",
       "      <td>Obligación</td>\n",
       "      <td>deber</td>\n",
       "      <td>deber</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4937</th>\n",
       "      <td>Obligación</td>\n",
       "      <td>acto impuesto por algo o alguien</td>\n",
       "      <td>imposición</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4938 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Palabra (concepto)                                        Descripción  \\\n",
       "0            Compasión                                         sentimiento   \n",
       "1            Compasión   por él que él él perdonar el vida a alguien en...   \n",
       "2                  plan                    necesario para cumplir objetivo   \n",
       "3                  plan                        organizar recurso o persona   \n",
       "4                  plan                                         estrategia   \n",
       "...                 ...                                                ...   \n",
       "4933         Obligación                                         imposicion   \n",
       "4934         Obligación                                  carácter extricto   \n",
       "4935         Obligación                                              norma   \n",
       "4936         Obligación                                              deber   \n",
       "4937         Obligación                   acto impuesto por algo o alguien   \n",
       "\n",
       "      Codificación  \n",
       "0      sentimiento  \n",
       "1           perdón  \n",
       "2        objetivos  \n",
       "3     organización  \n",
       "4       estrategia  \n",
       "...            ...  \n",
       "4933    imposición  \n",
       "4934      extricto  \n",
       "4935        normas  \n",
       "4936         deber  \n",
       "4937    imposición  \n",
       "\n",
       "[4938 rows x 3 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Data import\n",
    "# Relative Path of the dataset, change for your dataset\n",
    "dataset_name = \"cpn27\"\n",
    "file_path = ''\n",
    "\n",
    "if dataset_name == \"cpn27\":\n",
    "    # data = pd.read_csv(file_path+'CPN27_lemma.csv', delimiter=\",\")\n",
    "    data = pd.read_csv(file_path+'CPN27_lemma.csv', delimiter=\",\")\n",
    "    \n",
    "elif dataset_name == \"cpn120\":\n",
    "    data = pd.read_csv(file_path+'CPN120_lemma.csv', delimiter=\",\")\n",
    "\n",
    "# else:\n",
    "#     data = pd.read_csv(r'your-path/your-file.csv', delimiter=\",\")\n",
    "\n",
    "data.fillna(value='')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading .gitattributes: 100%|██████████| 1.48k/1.48k [00:00<?, ?B/s]\n",
      "Downloading 1_Pooling/config.json: 100%|██████████| 200/200 [00:00<?, ?B/s] \n",
      "Downloading README.md: 100%|██████████| 67.6k/67.6k [00:00<00:00, 510kB/s]\n",
      "Downloading config.json: 100%|██████████| 650/650 [00:00<00:00, 837kB/s]\n",
      "Downloading model.safetensors: 100%|██████████| 438M/438M [00:13<00:00, 33.0MB/s] \n",
      "Downloading onnx/config.json: 100%|██████████| 632/632 [00:00<?, ?B/s] \n",
      "Downloading model.onnx: 100%|██████████| 436M/436M [00:12<00:00, 36.2MB/s] \n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 125/125 [00:00<?, ?B/s] \n",
      "Downloading onnx/tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 1.32MB/s]\n",
      "Downloading (…)okenizer_config.json: 100%|██████████| 314/314 [00:00<?, ?B/s] \n",
      "Downloading onnx/vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 828kB/s]\n",
      "Downloading pytorch_model.bin: 100%|██████████| 438M/438M [00:10<00:00, 40.2MB/s] \n",
      "Downloading (…)nce_bert_config.json: 100%|██████████| 57.0/57.0 [00:00<?, ?B/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 125/125 [00:00<?, ?B/s] \n",
      "Downloading tokenizer.json: 100%|██████████| 711k/711k [00:00<00:00, 2.52MB/s]\n",
      "Downloading tokenizer_config.json: 100%|██████████| 314/314 [00:00<?, ?B/s] \n",
      "Downloading vocab.txt: 100%|██████████| 232k/232k [00:00<00:00, 7.65MB/s]\n",
      "Downloading modules.json: 100%|██████████| 387/387 [00:00<?, ?B/s] \n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('intfloat/e5-base-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceution time: 168.8829436302185\n"
     ]
    }
   ],
   "source": [
    "# Timer\n",
    "start = time.time()\n",
    "\n",
    "descriptions_matrix = np.zeros( # creatign an empty matrix\n",
    "    (\n",
    "        len(data),          # the number of data points\n",
    "        vector_length       # the number of components of the word embedding\n",
    "    )\n",
    ")\n",
    "\n",
    "# Matrix filling \n",
    "# Change to the name of the descriptions of your dataset.\n",
    "for i,description in enumerate(data.iloc[:,1]):\n",
    "    vector = model.encode(description)\n",
    "    descriptions_matrix[i,] = vector\n",
    "\n",
    "# Concatenate the matrix with the data of each observation\n",
    "data_matrix = np.concatenate([descriptions_matrix,data], axis=1)\n",
    "\n",
    "\n",
    "# Remove of the 'Nan' data\n",
    "data_matrix = data_matrix[~pd.isnull(data_matrix[:,:vector_length]).any(axis=1)]\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Exceution time:\", end-start)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AC-PLT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from AC_PLT import AC_PLT\n",
    "\n",
    "parameters = {'n_clusters': np.arange(50, 1800, 50)}\n",
    "ac_plt = AC_PLT()\n",
    "\n",
    "file_name = r'{}_{}_results.csv'.format(dataset_name, ac_plt.__class__.__name__)\n",
    "\n",
    "clf_acplt = ParamSearch(ac_plt, parameters)\n",
    "clf_acplt.fit(data_matrix[:, :vector_length], data_matrix[:, vector_length+2],file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Experimentations import ParamSearch\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "parameters = {'var_smoothing': np.geomspace(1e-06, 1e+02, num=9)}\n",
    "proc = GaussianNB()\n",
    "\n",
    "file_name = r'{}_{}_results.csv'.format(dataset_name, proc.__class__.__name__)\n",
    "\n",
    "clf = ParamSearch(proc, parameters)\n",
    "clf.fit(data_matrix[:, :vector_length], data_matrix[:, vector_length+2],file_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Experimentations import ParamSearch\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "kernel='rbf'\n",
    "parameters = {'C': np.geomspace(1e-05, 1e+01, num=7)}\n",
    "svc_rbf = SVC(kernel=kernel, gamma='auto')\n",
    "fileName = r'{}_{}{}_results.csv'.format(dataset_name, svc_rbf.__class__.__name__, kernel)\n",
    "\n",
    "clf_acplt = ParamSearch(svc_rbf, parameters)\n",
    "clf_acplt.fit(data_matrix[:, :vector_length], data_matrix[:, vector_length+2], fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Experimentations import ParamSearch\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "kernel='linear'\n",
    "parameters = {'C': np.geomspace(1e-05, 1e+01, num=7)}\n",
    "svc_linear = SVC(kernel=kernel, gamma='auto')\n",
    "fileName = r'{}_{}{}_results.csv'.format(dataset_name, svc_linear.__class__.__name__, kernel)\n",
    "\n",
    "\n",
    "clf_acplt = ParamSearch(svc_linear, parameters)\n",
    "clf_acplt.fit(data_matrix[:, :vector_length], data_matrix[:, vector_length+2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Experimentations import ParamSearch\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "kernel='poly'\n",
    "parameters = {'degree': np.arange(1,7)}\n",
    "svc_poly = SVC(kernel=kernel, gamma='auto')\n",
    "fileName = r'{}_{}{}_results.csv'.format(dataset_name, svc_poly.__class__.__name__, kernel)\n",
    "\n",
    "clf_acplt = ParamSearch(svc_poly, parameters)\n",
    "clf_acplt.fit(data_matrix[:, :vector_length], data_matrix[:, vector_length+2], fileName)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Experimentations import ParamSearch\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "kernel='sigmoid'\n",
    "parameters = {'C': np.geomspace(1e-05, 1e+01, num=7)}\n",
    "svc_sigmoid = SVC(kernel=kernel, gamma='auto')\n",
    "fileName = r'{}_{}{}_results.csv'.format(dataset_name, svc_rbf.__class__.__name__, kernel)\n",
    "\n",
    "clf_acplt = ParamSearch(svc_sigmoid, parameters)\n",
    "clf_acplt.fit(data_matrix[:, :vector_length], data_matrix[:, vector_length+2], fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Experimentations import ParamSearch\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "criterion = 'gini'\n",
    "parameters = {'max_leaf_nodes': np.arange(100, 1501, 100)}\n",
    "desition_tree = DecisionTreeClassifier(criterion=criterion)\n",
    "fileName = r'{}_{}_{}_results.csv'.format(dataset_name, desition_tree.__class__.__name__, criterion)\n",
    "\n",
    "clf_acplt = ParamSearch(desition_tree, parameters)\n",
    "clf_acplt.fit(data_matrix[:, :vector_length], data_matrix[:, vector_length+2], fileName)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cod = data.iloc[:,vector_length+2].value_counts()\n",
    "reduce_cod = cod[cod<5]\n",
    "n=5\n",
    "data_fill = data.copy()\n",
    "\n",
    "for key, value in reduce_cod.items():\n",
    "    m=np.abs(n-value)\n",
    "    nrow = np.zeros(vector_length)\n",
    "    nrow = np.concatenate([nrow, np.array(['', '', key])])\n",
    "    for i in range(m): data_fill = np.vstack([data_fill,nrow])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data_fill[:, vector_length+2]\n",
    "\n",
    "labels = np.unique(y)\n",
    "i=0\n",
    "idx2class = {}\n",
    "class2idx= {}\n",
    "for tp in labels:\n",
    "    idx2class[i] = tp\n",
    "    class2idx[tp] = i\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_label = np.vectorize(class2idx.get)(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_leaves = [10,50,100,500,1000,1500,2000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from Experimentations import ParamSearch\n",
    "\n",
    "parameters = {'max_leaves': max_leaves, 'n_estimators': np.arange(1, 20, 1)}\n",
    "\n",
    "bst = XGBClassifier(learning_rate=1, objective='multi:softprob', random_state=0)\n",
    "fileName = r'{}_{}_results.csv'.format(dataset_name, bst.__class__.__name__)\n",
    "\n",
    "clf_acplt = ParamSearch(bst, parameters)\n",
    "clf_acplt.fit(data_fill[:, :vector_length], y_label, fileName)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Experimentations import ParamSearch\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "parameters = {'n_neighbors': np.arange(10, 501, 10)}\n",
    "knn = KNeighborsClassifier()\n",
    "fileName = r'{}_{}_results.csv'.format(dataset_name, knn.__class__.__name__)\n",
    "\n",
    "clf_acplt = ParamSearch(knn, parameters)\n",
    "clf_acplt.fit(data.iloc[:, :vector_length].to_numpy(), data.iloc[:, vector_length+2].to_numpy(), fileName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
