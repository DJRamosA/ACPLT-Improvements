{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dra98\\OneDrive\\Documentos\\Trabajo\\Doctorado\\Codigo\\myvenv\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from scipy.stats import mode\n",
    "import sys\n",
    "import os\n",
    "\n",
    "sys.path.append(os.path.join(\"..\", \"..\"))\n",
    "\n",
    "# Word Embedding\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hydra import initialize, compose\n",
    "\n",
    "with initialize(version_base=None, config_path=\"../../conf\"):\n",
    "    cfg=compose(config_name=\"main.yaml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "# Relative Path of the dataset, change for your dataset\n",
    "\n",
    "dataset_name = \"cpn27\"\n",
    "# Options are \"cpn27\" and \"cpn120\"\n",
    "\n",
    "type_standardization = \"lemmatize_wo_stop\" \n",
    "folder_standardization = \"lemmatize\"\n",
    "# options are \"raw\", \"normalize\", \"normalize_wo_stop\", \"lemmatize\", and \"lemmatize_wo_stop\"\n",
    "\n",
    "# import of the data\n",
    "data = pd.read_csv(cfg.path[type_standardization][dataset_name], delimiter=\",\")\n",
    "data = data.fillna(value='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import spacy\n",
    "\n",
    "# Import of the model of the spanish billion words embeddings\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format( \n",
    "    cfg.path_embedding.word2vec, # Relative path of the vector\n",
    "    binary=True # The model is in binary format\n",
    ") \n",
    "\n",
    "nlp = spacy.load(cfg.enviroment.nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.27165865898132324\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dra98\\OneDrive\\Documentos\\Trabajo\\Doctorado\\Codigo\\myvenv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.49584843 0.54728613 0.56267693]\n",
      "[0.39590978 0.462333   0.48622949]\n"
     ]
    }
   ],
   "source": [
    "from functions.Vectorize import to_vector\n",
    "\n",
    "\n",
    "# Classification Model\n",
    "from functions.AC_PLT import AC_PLT\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Timer\n",
    "start = time.time()\n",
    "\n",
    "# Creation of a matrix full of 0 to save the vectors of each feature\n",
    "descriptions_matrix = np.zeros( \n",
    "    (\n",
    "        data.shape[0], # the number of data points\n",
    "        cfg.params.vector_length.word2vec  # the number of components of the word embedding\n",
    "    )\n",
    ")\n",
    "\n",
    "# Matrix filling with the vectors of each point\n",
    "for i,description in enumerate(data.iloc[:,1]):\n",
    "    descriptions_matrix[i,] = to_vector(description, model, cfg.params.vector_length.word2vec)\n",
    "\n",
    "# Concatenate the matrix with the data of each observation\n",
    "data_matrix = np.concatenate([descriptions_matrix,data], axis=1)\n",
    "\n",
    "# Remove of the 'Nan' values in the data\n",
    "data_matrix = data_matrix[~pd.isnull(data_matrix[:,:cfg.params.vector_length.word2vec]).any(axis=1)]\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Execution time:\", end-start)\n",
    "\n",
    "\n",
    "accuracies_tops_test = np.zeros((5,3))\n",
    "accuracies_tops_train = np.zeros((5,3))\n",
    "tops = (1, 3, 5)\n",
    "\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "X = data_matrix[:, :cfg.params.vector_length.word2vec] \n",
    "y = data_matrix[:, cfg.params.vector_length.word2vec+2]\n",
    "\n",
    "temp_test_acc = np.zeros(5)\n",
    "\n",
    "for c, (train_index, test_index) in enumerate(cross_validation.split(X, y)):\n",
    "\n",
    "    X_train = X[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    X_test = X[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    \n",
    "    classification_model = AC_PLT(n_clusters=cfg.params.kmeans.n_cluster[dataset_name])\n",
    "\n",
    "    classification_model.fit(X_train, y_train)\n",
    "\n",
    "    pred_ranking_test = classification_model.suggestions(X_test, n_codes=5)\n",
    "    pred_ranking_train = classification_model.suggestions(X_train, n_codes=5)\n",
    "\n",
    "    top1_acc_test = np.zeros(len(y_test))\n",
    "    top3_acc_test = np.zeros(len(y_test))\n",
    "    top5_acc_test = np.zeros(len(y_test))\n",
    "\n",
    "    \n",
    "    for j in range(len(y_test)):\n",
    "        top1_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[0]])\n",
    "        top3_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[1]])\n",
    "        top5_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[2]])\n",
    "\n",
    "\n",
    "    accuracies_tops_test[c] = [np.mean(top1_acc_test), np.mean(top3_acc_test), np.mean(top5_acc_test)]\n",
    "\n",
    "\n",
    "\n",
    "    top1_acc_train = np.zeros(len(y_train))\n",
    "    top3_acc_train = np.zeros(len(y_train))\n",
    "    top5_acc_train = np.zeros(len(y_train))\n",
    "\n",
    "    \n",
    "    for j in range(len(y_train)):\n",
    "        top1_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[0]])\n",
    "        top3_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[1]])\n",
    "        top5_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[2]])\n",
    "    \n",
    "    accuracies_tops_train[c] = [np.mean(top1_acc_train), np.mean(top3_acc_train), np.mean(top5_acc_train)]\n",
    "\n",
    "\n",
    "\n",
    "print(accuracies_tops_train.mean(axis=0))\n",
    "print(accuracies_tops_test.mean(axis=0))\n",
    "\n",
    "pd.DataFrame(\n",
    "    np.array([['top-1', 'top-3', 'top-5'], accuracies_tops_test.mean(axis=0), accuracies_tops_test.std(axis=0)]).T,\n",
    "    columns=['top position', 'mean accuracy', 'standar desviation']\n",
    "    ).to_csv(f'../../../data/experiment-top5/experiment_1/test/{folder_standardization}/word2vec_kmeans_{dataset_name}_{type_standardization}_top5.csv', index=False)\n",
    "\n",
    "pd.DataFrame(\n",
    "    np.array([['top-1', 'top-3', 'top-5'], accuracies_tops_train.mean(axis=0), accuracies_tops_train.std(axis=0)]).T,\n",
    "    columns=['top position', 'mean accuracy', 'standar desviation']\n",
    "    ).to_csv(f'../../../data/experiment-top5/experiment_1/train/{folder_standardization}/word2vec_kmeans_{dataset_name}_{type_standardization}_top5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data import\n",
    "# Relative Path of the dataset, change for your dataset\n",
    "\n",
    "dataset_name = \"cpn27\"\n",
    "# Options are \"cpn27\" and \"cpn120\"\n",
    "\n",
    "type_standardization = \"lemmatize\" \n",
    "# options are \"raw\", \"normalize\", \"normalize_wo_stop\", and \"lemmatize\"\n",
    "\n",
    "# import of the data\n",
    "data = pd.read_csv(cfg.path[type_standardization][dataset_name], delimiter=\",\")\n",
    "data = data.fillna(value='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LaBSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceution time: 140.41847324371338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dra98\\OneDrive\\Documentos\\Trabajo\\Doctorado\\Codigo\\myvenv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4442085  0.50091147 0.52293461]\n",
      "[0.31733651 0.39712764 0.43135211]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('sentence-transformers/LaBSE')\n",
    "\n",
    "# Timer\n",
    "start = time.time()\n",
    "\n",
    "descriptions_matrix = np.zeros( # creatign an empty matrix\n",
    "    (\n",
    "        len(data),                                       # the number of data points\n",
    "        cfg.params.vector_length.sentence_embedding      # the number of components of the word embedding\n",
    "    )\n",
    ")\n",
    "\n",
    "# Matrix filling \n",
    "# Change to the name of the descriptions of your dataset.\n",
    "for i,description in enumerate(data.iloc[:,1]):\n",
    "    vector = model.encode(description)\n",
    "    descriptions_matrix[i,] = vector\n",
    "\n",
    "# Concatenate the matrix with the data of each observation\n",
    "data_matrix = np.concatenate([descriptions_matrix,data], axis=1)\n",
    "\n",
    "\n",
    "# Remove of the 'Nan' data\n",
    "data_matrix = data_matrix[~pd.isnull(data_matrix[:,:cfg.params.vector_length.sentence_embedding ]).any(axis=1)]\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Exceution time:\", end-start)\n",
    "\n",
    "\n",
    "# Classification Model\n",
    "from functions.AC_PLT import AC_PLT\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "accuracies_tops_test = np.zeros((5,3))\n",
    "accuracies_tops_train = np.zeros((5,3))\n",
    "tops = (1, 3, 5)\n",
    "\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "X = data_matrix[:, :cfg.params.vector_length.sentence_embedding] \n",
    "y = data_matrix[:, cfg.params.vector_length.sentence_embedding+2]\n",
    "\n",
    "temp_test_acc = np.zeros(5)\n",
    "\n",
    "for c, (train_index, test_index) in enumerate(cross_validation.split(X, y)):\n",
    "\n",
    "    X_train = X[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    X_test = X[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    \n",
    "    classification_model = AC_PLT(n_clusters=cfg.params.kmeans.n_cluster[dataset_name])\n",
    "\n",
    "    classification_model.fit(X_train, y_train)\n",
    "\n",
    "    pred_ranking_test = classification_model.suggestions(X_test, n_codes=5)\n",
    "\n",
    "    top1_acc_test = np.zeros(len(y_test))\n",
    "    top3_acc_test = np.zeros(len(y_test))\n",
    "    top5_acc_test = np.zeros(len(y_test))\n",
    "\n",
    "    \n",
    "    for j in range(len(y_test)):\n",
    "        top1_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[0]])\n",
    "        top3_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[1]])\n",
    "        top5_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[2]])\n",
    "\n",
    "\n",
    "    accuracies_tops_test[c] = [np.mean(top1_acc_test), np.mean(top3_acc_test), np.mean(top5_acc_test)]\n",
    "\n",
    "\n",
    "    pred_ranking_train = classification_model.suggestions(X_train, n_codes=5)\n",
    "\n",
    "    top1_acc_train = np.zeros(len(y_train))\n",
    "    top3_acc_train = np.zeros(len(y_train))\n",
    "    top5_acc_train = np.zeros(len(y_train))\n",
    "\n",
    "    \n",
    "    for j in range(len(y_train)):\n",
    "        top1_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[0]])\n",
    "        top3_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[1]])\n",
    "        top5_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[2]])\n",
    "    \n",
    "    accuracies_tops_train[c] = [np.mean(top1_acc_train), np.mean(top3_acc_train), np.mean(top5_acc_train)]\n",
    "\n",
    "\n",
    "\n",
    "print(accuracies_tops_train.mean(axis=0))\n",
    "print(accuracies_tops_test.mean(axis=0))\n",
    "\n",
    "pd.DataFrame(\n",
    "    np.array([['top-1', 'top-3', 'top-5'], accuracies_tops_test.mean(axis=0), accuracies_tops_test.std(axis=0)]).T,\n",
    "    columns=['top position', 'mean accuracy', 'standar desviation']\n",
    "    ).to_csv(f'../../../data/experiment-top5/experiment_1/test/{type_standardization}/LaBSE_kmeans_{dataset_name}_{type_standardization}_top5.csv', index=False)\n",
    "\n",
    "pd.DataFrame(\n",
    "    np.array([['top-1', 'top-3', 'top-5'], accuracies_tops_train.mean(axis=0), accuracies_tops_train.std(axis=0)]).T,\n",
    "    columns=['top position', 'mean accuracy', 'standar desviation']\n",
    "    ).to_csv(f'../../../data/experiment-top5/experiment_1/train/{type_standardization}/LaBSE_kmeans_{dataset_name}_{type_standardization}_top5.csv', index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BETO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name C:\\Users\\dra98/.cache\\torch\\sentence_transformers\\dccuchile_bert-base-spanish-wwm-cased. Creating a new one with MEAN pooling.\n",
      "Some weights of BertModel were not initialized from the model checkpoint at C:\\Users\\dra98/.cache\\torch\\sentence_transformers\\dccuchile_bert-base-spanish-wwm-cased and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceution time: 142.71896767616272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dra98\\OneDrive\\Documentos\\Trabajo\\Doctorado\\Codigo\\myvenv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.36750736 0.39241634 0.40689587]\n",
      "[0.22904417 0.26630713 0.29182264]\n"
     ]
    }
   ],
   "source": [
    "# Classification Model\n",
    "from functions.AC_PLT import AC_PLT\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "model = SentenceTransformer('dccuchile/bert-base-spanish-wwm-cased')\n",
    "\n",
    "# Timer\n",
    "start = time.time()\n",
    "\n",
    "descriptions_matrix = np.zeros( # creatign an empty matrix\n",
    "    (\n",
    "        len(data),                                       # the number of data points\n",
    "        cfg.params.vector_length.sentence_embedding      # the number of components of the word embedding\n",
    "    )\n",
    ")\n",
    "\n",
    "# Matrix filling \n",
    "# Change to the name of the descriptions of your dataset.\n",
    "for i,description in enumerate(data.iloc[:,1]):\n",
    "    vector = model.encode(description)\n",
    "    descriptions_matrix[i,] = vector\n",
    "\n",
    "# Concatenate the matrix with the data of each observation\n",
    "data_matrix = np.concatenate([descriptions_matrix,data], axis=1)\n",
    "\n",
    "\n",
    "# Remove of the 'Nan' data\n",
    "data_matrix = data_matrix[~pd.isnull(data_matrix[:,:cfg.params.vector_length.sentence_embedding ]).any(axis=1)]\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Exceution time:\", end-start)\n",
    "\n",
    "accuracies_tops_test = np.zeros((5,3))\n",
    "accuracies_tops_train = np.zeros((5,3))\n",
    "tops = (1, 3, 5)\n",
    "\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "X = data_matrix[:, :cfg.params.vector_length.sentence_embedding] \n",
    "y = data_matrix[:, cfg.params.vector_length.sentence_embedding+2]\n",
    "\n",
    "temp_test_acc = np.zeros(5)\n",
    "\n",
    "for c, (train_index, test_index) in enumerate(cross_validation.split(X, y)):\n",
    "\n",
    "    X_train = X[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    X_test = X[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    \n",
    "    classification_model = AC_PLT(n_clusters=cfg.params.kmeans.n_cluster[dataset_name])\n",
    "\n",
    "    classification_model.fit(X_train, y_train)\n",
    "\n",
    "    pred_ranking_test = classification_model.suggestions(X_test, n_codes=5)\n",
    "    pred_ranking_train = classification_model.suggestions(X_train, n_codes=5)\n",
    "\n",
    "    top1_acc_test = np.zeros(len(y_test))\n",
    "    top3_acc_test = np.zeros(len(y_test))\n",
    "    top5_acc_test = np.zeros(len(y_test))\n",
    "\n",
    "    \n",
    "    for j in range(len(y_test)):\n",
    "        top1_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[0]])\n",
    "        top3_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[1]])\n",
    "        top5_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[2]])\n",
    "\n",
    "\n",
    "    accuracies_tops_test[c] = [np.mean(top1_acc_test), np.mean(top3_acc_test), np.mean(top5_acc_test)]\n",
    "\n",
    "\n",
    "\n",
    "    top1_acc_train = np.zeros(len(y_train))\n",
    "    top3_acc_train = np.zeros(len(y_train))\n",
    "    top5_acc_train = np.zeros(len(y_train))\n",
    "\n",
    "    \n",
    "    for j in range(len(y_train)):\n",
    "        top1_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[0]])\n",
    "        top3_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[1]])\n",
    "        top5_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[2]])\n",
    "    \n",
    "    accuracies_tops_train[c] = [np.mean(top1_acc_train), np.mean(top3_acc_train), np.mean(top5_acc_train)]\n",
    "\n",
    "\n",
    "\n",
    "print(accuracies_tops_train.mean(axis=0))\n",
    "print(accuracies_tops_test.mean(axis=0))\n",
    "\n",
    "pd.DataFrame(\n",
    "    np.array([['top-1', 'top-3', 'top-5'], accuracies_tops_test.mean(axis=0), accuracies_tops_test.std(axis=0)]).T,\n",
    "    columns=['top position', 'mean accuracy', 'standar desviation']\n",
    "    ).to_csv(f'../../../data/experiment-top5/experiment_1/test/{type_standardization}/BETO_kmeans_{dataset_name}_{type_standardization}_top5.csv', index=False)\n",
    "\n",
    "pd.DataFrame(\n",
    "    np.array([['top-1', 'top-3', 'top-5'], accuracies_tops_train.mean(axis=0), accuracies_tops_train.std(axis=0)]).T,\n",
    "    columns=['top position', 'mean accuracy', 'standar desviation']\n",
    "    ).to_csv(f'../../../data/experiment-top5/experiment_1/train/{type_standardization}/BETO_kmeans_{dataset_name}_{type_standardization}_top5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exceution time: 160.47151398658752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dra98\\OneDrive\\Documentos\\Trabajo\\Doctorado\\Codigo\\myvenv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.51255598 0.5705247  0.59052264]\n",
      "[0.40178371 0.4896755  0.51822888]\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('intfloat/e5-base-v2')\n",
    "\n",
    "# Timer\n",
    "start = time.time()\n",
    "\n",
    "descriptions_matrix = np.zeros( # creatign an empty matrix\n",
    "    (\n",
    "        len(data),                                       # the number of data points\n",
    "        cfg.params.vector_length.sentence_embedding      # the number of components of the word embedding\n",
    "    )\n",
    ")\n",
    "\n",
    "# Matrix filling \n",
    "# Change to the name of the descriptions of your dataset.\n",
    "for i,description in enumerate(data.iloc[:,1]):\n",
    "    vector = model.encode(description)\n",
    "    descriptions_matrix[i,] = vector\n",
    "\n",
    "# Concatenate the matrix with the data of each observation\n",
    "data_matrix = np.concatenate([descriptions_matrix,data], axis=1)\n",
    "\n",
    "\n",
    "# Remove of the 'Nan' data\n",
    "data_matrix = data_matrix[~pd.isnull(data_matrix[:,:cfg.params.vector_length.sentence_embedding ]).any(axis=1)]\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Exceution time:\", end-start)\n",
    "\n",
    "\n",
    "# Classification Model\n",
    "from functions.AC_PLT import AC_PLT\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "accuracies_tops_test = np.zeros((5,3))\n",
    "accuracies_tops_train = np.zeros((5,3))\n",
    "tops = (1, 3, 5)\n",
    "\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "X = data_matrix[:, :cfg.params.vector_length.sentence_embedding] \n",
    "y = data_matrix[:, cfg.params.vector_length.sentence_embedding+2]\n",
    "\n",
    "temp_test_acc = np.zeros(5)\n",
    "\n",
    "for c, (train_index, test_index) in enumerate(cross_validation.split(X, y)):\n",
    "\n",
    "    X_train = X[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    X_test = X[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    \n",
    "    classification_model = AC_PLT(n_clusters=cfg.params.kmeans.n_cluster[dataset_name])\n",
    "\n",
    "    classification_model.fit(X_train, y_train)\n",
    "\n",
    "    pred_ranking_test = classification_model.suggestions(X_test, n_codes=5)\n",
    "    pred_ranking_train = classification_model.suggestions(X_train, n_codes=5)\n",
    "\n",
    "    top1_acc_test = np.zeros(len(y_test))\n",
    "    top3_acc_test = np.zeros(len(y_test))\n",
    "    top5_acc_test = np.zeros(len(y_test))\n",
    "\n",
    "    \n",
    "    for j in range(len(y_test)):\n",
    "        top1_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[0]])\n",
    "        top3_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[1]])\n",
    "        top5_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[2]])\n",
    "\n",
    "\n",
    "    accuracies_tops_test[c] = [np.mean(top1_acc_test), np.mean(top3_acc_test), np.mean(top5_acc_test)]\n",
    "\n",
    "\n",
    "\n",
    "    top1_acc_train = np.zeros(len(y_train))\n",
    "    top3_acc_train = np.zeros(len(y_train))\n",
    "    top5_acc_train = np.zeros(len(y_train))\n",
    "\n",
    "    \n",
    "    for j in range(len(y_train)):\n",
    "        top1_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[0]])\n",
    "        top3_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[1]])\n",
    "        top5_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[2]])\n",
    "    \n",
    "    accuracies_tops_train[c] = [np.mean(top1_acc_train), np.mean(top3_acc_train), np.mean(top5_acc_train)]\n",
    "\n",
    "\n",
    "\n",
    "print(accuracies_tops_train.mean(axis=0))\n",
    "print(accuracies_tops_test.mean(axis=0))\n",
    "\n",
    "pd.DataFrame(\n",
    "    np.array([['top-1', 'top-3', 'top-5'], accuracies_tops_test.mean(axis=0), accuracies_tops_test.std(axis=0)]).T,\n",
    "    columns=['top position', 'mean accuracy', 'standar desviation']\n",
    "    ).to_csv(f'../../../data/experiment-top5/experiment_1/test/{type_standardization}/E5_kmeans_{dataset_name}_{type_standardization}_top5.csv', index=False)\n",
    "\n",
    "pd.DataFrame(\n",
    "    np.array([['top-1', 'top-3', 'top-5'], accuracies_tops_train.mean(axis=0), accuracies_tops_train.std(axis=0)]).T,\n",
    "    columns=['top position', 'mean accuracy', 'standar desviation']\n",
    "    ).to_csv(f'../../../data/experiment-top5/experiment_1/train/{type_standardization}/E5_kmeans_{dataset_name}_{type_standardization}_top5.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CANINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrir csv con vectores numericos.\n",
    "\n",
    "dataset_name = \"cpn27\"\n",
    "# Options are \"cpn27\" and \"cpn120\"\n",
    "\n",
    "type_standardization = \"lemmatize\" \n",
    "# options are \"raw\", \"normalize\", \"normalize_wo_stop\", and \"lemmatize\"\n",
    "\n",
    "if type_standardization == \"lemmatize\":\n",
    "    data_matrix = pd.read_csv(cfg.path_embedding_canine[dataset_name][type_standardization], delimiter=\",\",index_col=\"Unnamed: 0\").to_numpy()\n",
    "else:\n",
    "    data_matrix = pd.read_csv(cfg.path_embedding_canine[dataset_name][type_standardization], delimiter=\",\").to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\dra98\\OneDrive\\Documentos\\Trabajo\\Doctorado\\Codigo\\myvenv\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.30052671 0.30579202 0.31065221]\n",
      "[0.16464443 0.17375722 0.18104857]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Classification Model\n",
    "from functions.AC_PLT import AC_PLT\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "\n",
    "\n",
    "accuracies_tops_test = np.zeros((5,3))\n",
    "accuracies_tops_train = np.zeros((5,3))\n",
    "tops = (1, 3, 5)\n",
    "\n",
    "\n",
    "cross_validation = StratifiedKFold(n_splits = 5)\n",
    "\n",
    "X = data_matrix[:, :cfg.params.vector_length.sentence_embedding] \n",
    "y = data_matrix[:, cfg.params.vector_length.sentence_embedding+2]\n",
    "\n",
    "temp_test_acc = np.zeros(5)\n",
    "\n",
    "for c, (train_index, test_index) in enumerate(cross_validation.split(X, y)):\n",
    "\n",
    "    X_train = X[train_index, :]\n",
    "    y_train = y[train_index]\n",
    "\n",
    "    X_test = X[test_index, :]\n",
    "    y_test = y[test_index]\n",
    "    \n",
    "    \n",
    "    classification_model = AC_PLT(n_clusters=cfg.params.kmeans.n_cluster[dataset_name])\n",
    "\n",
    "    classification_model.fit(X_train, y_train)\n",
    "\n",
    "    pred_ranking_test = classification_model.suggestions(X_test, n_codes=5)\n",
    "    pred_ranking_train = classification_model.suggestions(X_train, n_codes=5)\n",
    "\n",
    "    top1_acc_test = np.zeros(len(y_test))\n",
    "    top3_acc_test = np.zeros(len(y_test))\n",
    "    top5_acc_test = np.zeros(len(y_test))\n",
    "\n",
    "    \n",
    "    for j in range(len(y_test)):\n",
    "        top1_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[0]])\n",
    "        top3_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[1]])\n",
    "        top5_acc_test[j] = int(y_test[j] in pred_ranking_test[j, :tops[2]])\n",
    "\n",
    "\n",
    "    accuracies_tops_test[c] = [np.mean(top1_acc_test), np.mean(top3_acc_test), np.mean(top5_acc_test)]\n",
    "\n",
    "\n",
    "\n",
    "    top1_acc_train = np.zeros(len(y_train))\n",
    "    top3_acc_train = np.zeros(len(y_train))\n",
    "    top5_acc_train = np.zeros(len(y_train))\n",
    "\n",
    "    \n",
    "    for j in range(len(y_train)):\n",
    "        top1_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[0]])\n",
    "        top3_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[1]])\n",
    "        top5_acc_train[j] = int(y_train[j] in pred_ranking_train[j, :tops[2]])\n",
    "    \n",
    "    accuracies_tops_train[c] = [np.mean(top1_acc_train), np.mean(top3_acc_train), np.mean(top5_acc_train)]\n",
    "\n",
    "\n",
    "\n",
    "print(accuracies_tops_train.mean(axis=0))\n",
    "print(accuracies_tops_test.mean(axis=0))\n",
    "\n",
    "pd.DataFrame(\n",
    "    np.array([['top-1', 'top-3', 'top-5'], accuracies_tops_test.mean(axis=0), accuracies_tops_test.std(axis=0)]).T,\n",
    "    columns=['top position', 'mean accuracy', 'standar desviation']\n",
    "    ).to_csv(f'../../../data/experiment-top5/experiment_1/test/{type_standardization}/canine_kmeans_{dataset_name}_{type_standardization}_top5.csv', index=False)\n",
    "\n",
    "pd.DataFrame(\n",
    "    np.array([['top-1', 'top-3', 'top-5'], accuracies_tops_train.mean(axis=0), accuracies_tops_train.std(axis=0)]).T,\n",
    "    columns=['top position', 'mean accuracy', 'standar desviation']\n",
    "    ).to_csv(f'../../../data/experiment-top5/experiment_1/train/{type_standardization}/canine_kmeans_{dataset_name}_{type_standardization}_top5.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "550"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfg.params.kmeans.n_cluster[dataset_name]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myvenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
